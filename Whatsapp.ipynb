{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1712475910534,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "LRkBYda8TD2b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1712475912551,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "eNx-KMdFTrjP"
   },
   "outputs": [],
   "source": [
    "f = open(\"WhatsApp Chat.txt\",\"r\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1712475920574,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "Kf5Dj0W2T0tD"
   },
   "outputs": [],
   "source": [
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXwlqw_1T2vt"
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1712475923206,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "-H9CKFOdUD00"
   },
   "outputs": [],
   "source": [
    "pattern = '\\d{1,2}/\\d{1,2}/\\d{2,4},\\s\\d{1,2}:\\d{2}\\s-\\s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEgHE1vKUIbJ"
   },
   "outputs": [],
   "source": [
    "messages = re.split(pattern,data)[1:]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mp9ZUp4kUN0m"
   },
   "outputs": [],
   "source": [
    "dates = re.findall(pattern,data)\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pct9gmguU3pN"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'user_message':messages,\"message_date\":dates})\n",
    "\n",
    "df['message_date'] = pd.to_datetime(df['message_date'].str.rstrip(), format='%d/%m/%y, %H:%M -')\n",
    "\n",
    "\n",
    "df.rename(columns={'message_date':'date'},inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1712475933324,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "4QN_hMZcVf0o",
    "outputId": "1ab6fb47-771f-4de6-d50a-d10cd758f6e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10095, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4123,
     "status": "ok",
     "timestamp": 1712475939188,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "OjrB4YfPWvAC"
   },
   "outputs": [],
   "source": [
    "users = []\n",
    "messages = []\n",
    "for message in df['user_message']:\n",
    "  entry = re.split('([\\w\\W]+?):\\s',message)\n",
    "  if entry[1:]:\n",
    "    users.append(entry[1])\n",
    "    messages.append(entry[2])\n",
    "  else:\n",
    "    users.append('group_notification')\n",
    "    messages.append(entry[0])\n",
    "\n",
    "df['user'] = users\n",
    "df ['message'] = messages\n",
    "df.drop(columns=['user_message'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi0xs3sGXu8_"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1712475939189,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "IfbN8PAgXyA7"
   },
   "outputs": [],
   "source": [
    "df['year'] = df['date'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1712475939190,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "LwOhaVizYMZ2"
   },
   "outputs": [],
   "source": [
    "df['month'] = df['date'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1712475939190,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "m2WSEn51YWz3"
   },
   "outputs": [],
   "source": [
    "df['day'] = df['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712475941067,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "Ran-U6o3Yfbg"
   },
   "outputs": [],
   "source": [
    "df['hour'] = df['date'].dt.hour\n",
    "df['minute'] = df['date'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlD09Gj5Yr3C"
   },
   "outputs": [],
   "source": [
    "# Import TextBlob for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Perform sentiment analysis and add a sentiment column to the DataFrame\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment'] = df['message'].apply(get_sentiment)\n",
    "\n",
    "# Map sentiment labels to numeric values\n",
    "sentiment_mapping = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "df['sentiment_numeric'] = df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Display the DataFrame with the new sentiment column\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZf6nqdonS1h"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"\")\n",
    "\n",
    "# Concatenate all messages into a single string\n",
    "all_messages = \"\\n\".join(df['message'])\n",
    "\n",
    "\n",
    "# Split input text into chunks\n",
    "chunk_size = 1000  # You can adjust this value based on the maximum input size supported by the model\n",
    "chunks = [all_messages[i:i+chunk_size] for i in range(0, len(all_messages), chunk_size)]\n",
    "\n",
    "# Set maximum length of the summary\n",
    "max_length = 100\n",
    "\n",
    "# Generate summaries for each chunk\n",
    "summaries = []\n",
    "for chunk in chunks:\n",
    "    summary = summarizer(chunk, max_length=max_length, min_length=30, do_sample=False)\n",
    "    summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "print(\"Summary:\", summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1712475954990,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "EI0iDlD4nYHe"
   },
   "outputs": [],
   "source": [
    "f = open('stop_hinglish.txt','r')\n",
    "stop_words = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRzUcsLF3P4e"
   },
   "outputs": [],
   "source": [
    "# Combine messages by user and date\n",
    "combined_messages = df.groupby(['user', 'date'])['message'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "combined_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2625,
     "status": "ok",
     "timestamp": 1712475964190,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "anBiacY73513",
    "outputId": "385f5b79-984e-47aa-f477-0411e9e19953"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911-0plj5ukz"
   },
   "outputs": [],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5267,
     "status": "ok",
     "timestamp": 1712476545556,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "ecJYlZJ_3piF",
    "outputId": "38c87bb1-b5ca-4ca5-8e0a-bf443ddb3b1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-43077cd983d2>:23: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove unnecessary characters and convert to lowercase\n",
    "    cleaned_words = [' '.join(word.lower().split()) for word in words if word.isalnum()]\n",
    "    # Perform stemming and remove stop words\n",
    "    cleaned_words = [ps.stem(word) for word in cleaned_words if word not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "combined_messages['processed_message'] = combined_messages['message'].apply(preprocess_text)\n",
    "# Tokenize the text into sentences\n",
    "sentences = combined_messages['processed_message'].apply(sent_tokenize)\n",
    "\n",
    "import demoji\n",
    "\n",
    "# Download emoji data (only need to run once)\n",
    "demoji.download_codes()\n",
    "\n",
    "# Function to remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, '')\n",
    "\n",
    "# Apply emoji removal to the 'processed_message' column\n",
    "combined_messages['processed_message'] = combined_messages['processed_message'].apply(remove_emojis)\n",
    "\n",
    "# Format data for text summarization\n",
    "documents = combined_messages['processed_message'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69aw8fKc3_iM"
   },
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712476555652,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "tcewtsud6NRT"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712476558275,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "o7EM68lPh5xb",
    "outputId": "c995db0f-1633-46a0-d91e-37f72ac050fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0023', '0116', '013', ..., 'पहन', 'भवन', 'वह'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1712476560018,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "tm7_DH0FiCOC",
    "outputId": "dc82a4cb-375d-43e1-9291-a2eb52e8ac68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4184, 3585)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1712476615791,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "WizDGvKgju8P"
   },
   "outputs": [],
   "source": [
    "sentences = sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1712477399314,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "b7gkIQ63iezn",
    "outputId": "1c6e4033-0eda-4350-83ef-0b82581f6ad3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "s\n",
      "e\n",
      "e\n",
      "d\n",
      "h\n",
      "a\n",
      "\n",
      "Cluster 2:\n",
      "w\n",
      "a\n",
      "i\n",
      "t\n",
      " \n",
      "m\n",
      "e\n",
      "s\n",
      "s\n",
      "a\n",
      "g\n",
      "\n",
      "Cluster 3:\n",
      "m\n",
      "e\n",
      "s\n",
      "s\n",
      "a\n",
      "g\n",
      " \n",
      "d\n",
      "e\n",
      "l\n",
      "e\n",
      "t\n",
      "\n",
      "Cluster 4:\n",
      "\n",
      "Cluster 5:\n",
      "m\n",
      "e\n",
      "d\n",
      "i\n",
      "a\n",
      " \n",
      "o\n",
      "m\n",
      "i\n",
      "t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "# Use KMeans clustering to identify representative sentences\n",
    "n_clusters = 5  # Number of clusters (can be adjusted)\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Find sentences closest to cluster centers\n",
    "representative_sentences = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == i)[0]\n",
    "    centroid_distances = np.linalg.norm(tfidf_matrix[cluster_indices] - cluster_centers[i], axis=1)\n",
    "    representative_idx = cluster_indices[np.argmin(centroid_distances)]\n",
    "    representative_sentences.append(documents[representative_idx])\n",
    "\n",
    "# Print representative sentences for each cluster\n",
    "for i, sentences in enumerate(representative_sentences):\n",
    "    print(f\"Cluster {i+1}:\")\n",
    "    for sent in sentences:\n",
    "        print(sent)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1712477770995,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "73oLDE3HkNIV",
    "outputId": "65ba190e-37f3-4f31-86c5-faeb53653437"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1712477833116,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "evWF3wa0kyrr"
   },
   "outputs": [],
   "source": [
    "def generate_summary(document):\n",
    "\n",
    "    # Tokenize the document into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "\n",
    "    # Clean and preprocess the sentences (similar to the preprocessing steps used during training)\n",
    "    cleaned_sentences = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent.lower())\n",
    "        # Remove unnecessary characters and convert to lowercase\n",
    "        cleaned_words = [' '.join(word.lower().split()) for word in words if word.isalnum()]\n",
    "        # Perform stemming and remove stop words\n",
    "        cleaned_words = [ps.stem(word) for word in cleaned_words if word not in stop_words]\n",
    "        cleaned_sentences.append(' '.join(cleaned_words))\n",
    "\n",
    "    # Convert cleaned sentences into TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.transform(cleaned_sentences)\n",
    "\n",
    "    if tfidf_matrix.shape[0] == 0:\n",
    "        raise ValueError(\"TF-IDF matrix is empty\")\n",
    "    # Predict cluster labels for the sentences using the trained KMeans model\n",
    "    cluster_labels = kmeans.predict(tfidf_matrix)\n",
    "    print(cluster_labels)\n",
    "    # Select representative sentences from each cluster\n",
    "    representative_sentences = []\n",
    "    for i in range(3,4):\n",
    "        print(i)\n",
    "        cluster_indices = np.where(cluster_labels == i)\n",
    "        print(cluster_indices)\n",
    "        centroid_distances = np.linalg.norm(tfidf_matrix[cluster_indices] - cluster_centers[i], axis=1)\n",
    "        representative_idx = cluster_indices[np.argmin(centroid_distances)]\n",
    "        representative_sentences.append(sentences[representative_idx])\n",
    "\n",
    "    # Concatenate representative sentences to form the summary\n",
    "    summary = ' '.join(representative_sentences)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1712477834541,
     "user": {
      "displayName": "Daksh Jain",
      "userId": "16209101031814765558"
     },
     "user_tz": -330
    },
    "id": "d70jcFtWlWwJ",
    "outputId": "9da349bd-4871-4973-b15d-94e236dba357"
   },
   "outputs": [],
   "source": [
    "# Sample document\n",
    "document = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "NLP tasks include text translation, sentiment analysis, speech recognition, and text summarization. Text summarization aims to condense a piece of text while preserving its key information and meaning.\n",
    "\n",
    "There are two main approaches to text summarization: extractive and abstractive. Extractive summarization involves selecting and concatenating important sentences or phrases from the original text. Abstractive summarization involves generating new sentences that capture the essence of the original text.\n",
    "\n",
    "In this example, we will use an extractive summarization approach based on KMeans clustering. We will tokenize the text, compute TF-IDF vectors for the sentences, and use KMeans to cluster the sentences into representative groups. Then, we will select one representative sentence from each cluster to form the summary.\n",
    "\"\"\"\n",
    "\n",
    "# Test generate_summary function\n",
    "summary = generate_summary(document)\n",
    "\n",
    "# Print generated summary\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPVFViEC0P3PotHLX2xKeo7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
